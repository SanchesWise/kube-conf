Это отличный момент для модернизации. Ввод третьей ноды — идеальное время для обновления мажорной версии, так как у вас появляется "запасной аэродром".

**Важное уточнение:** На текущий момент (декабрь 2025 года в контексте вашего промпта, но фактически сейчас) **Proxmox VE 9 ещё не существует**. Актуальная стабильная версия — **Proxmox VE 8.x** (основана на Debian 12 Bookworm). Proxmox 7.4 (Debian 11 Bullseye) — это предыдущая стабильная ветка.
*Инструкция ниже написана для обновления до **Proxmox VE 8** (последней актуальной).*

---

### Стратегия миграции: "Rolling Upgrade"
Мы не будем ломать кластер. Мы обновим существующие ноды по очереди, а новую ноду сразу установим на свежую версию 8.x.

**Порядок действий:**
1.  **Подготовка:** Бэкапы, проверка здоровья текущего кластера.
2.  **Обновление существующего кластера:** Supermicro Node 2 -> Supermicro Node 1.
3.  **Ввод новой ноды:** Чистая установка PVE 8 на Huananzhi и ввод в кластер.

---

### ЭТАП 1: Подготовка и Бэкапы (Критично!)

Перед тем как трогать `apt dist-upgrade`, убедитесь, что у вас есть путь к отступлению.

1.  **Бэкап виртуальных машин:**
    *   Сделайте **VZDump** (полные бэкапы) всех критичных VM (K8s Master, NFS server, DB) на внешнее хранилище (ваше NFS `managed-nfs-storage` или HDD).
    *   Проверьте, что Velero внутри K8s тоже отработал.
2.  **Проверка сети:**
    *   Убедитесь, что у вас есть доступ к нодам через **IPMI/BMC** (на Supermicro) или физический монитор/клавиатуру. Если сеть отвалится при обновлении, SSH вас не спасет.
3.  **Кворум (Corosync):**
    *   Так как у вас сейчас 2 ноды, при перезагрузке одной вторая потеряет кворум и перейдет в Read-Only режим (нельзя будет править конфиги, стартовать VM).
    *   Перед началом работ на каждой ноде выполните:
        ```bash
        pvecm expected 1
        ```
        *Это временно разрешит работу кластера с одной живой нодой.*

---

### ЭТАП 2: Обновление Proxmox 7.4 -> 8.x (Существующие ноды)

Делаем по очереди. Сначала **k8s-worker** ноду (Supermicro #2), затем **k8s-control** ноду (Supermicro #1).

#### Шаг 2.1: Освобождение ноды
1.  Мигрируйте все запущенные VM с обновляемой ноды на другую.
2.  В Kubernetes предварительно сделайте `kubectl drain <node-name> --ignore-daemonsets`, чтобы поды корректно переехали.

#### Шаг 2.2: Обновление системы (на обновляемой ноде)
1.  Убедитесь, что стоит последняя версия 7.4:
    ```bash
    apt update && apt dist-upgrade -y
    ```
2.  Запустите скрипт проверки совместимости:
    ```bash
    pve7to8
    ```
    *   **Исправьте все FAIL и WARNING.** Если скрипт ругается на DKMS модули (старые драйвера Nvidia и т.д.) — удалите их. Мы поставим новые на 8-ке.
3.  Меняем репозитории с Bullseye (7) на Bookworm (8):
    ```bash
    sed -i 's/bullseye/bookworm/g' /etc/apt/sources.list
    sed -i 's/bullseye/bookworm/g' /etc/apt/sources.list.d/*
    ```
    *Если используете Enterprise репозиторий (платный), убедитесь, что он тоже обновлен. Если No-Subscription — проверьте, что он прописан корректно.*
    
    Пример `/etc/apt/sources.list.d/pve-no-subscription.list` для PVE 8:
    ```
    deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription
    ```
4.  Обновляем Ceph (если используется):
    *   Если у вас стоит Ceph Pacific (16) или Quincy (17), добавьте репозитории Ceph для Bookworm. (Если Ceph не используется, пропустите).

#### Шаг 2.3: Сам процесс обновления
1.  Обновляем списки пакетов:
    ```bash
    apt update
    ```
2.  Запускаем обновление дистрибутива:
    ```bash
    apt dist-upgrade
    ```
    *   **Важно:** В процессе он будет спрашивать про файлы конфигурации (`/etc/issue`, `/etc/ssh/sshd_config` и т.д.).
    *   Обычно безопасно выбирать **"N" (keep your currently-installed version)**, если вы правили конфиги вручную. Если не уверены — смотрите diff.
3.  После завершения — **перезагрузка**:
    ```bash
    reboot
    ```

#### Шаг 2.4: Проверка после перезагрузки
1.  `pveversion` — должна показать 8.x.
2.  Проверьте сеть (LACP бонды). В Debian 12 изменился формат имен интерфейсов (иногда), но Proxmox обычно сохраняет старые.
3.  Если ZFS используется: `zpool status -v`. (Пока не делайте `zpool upgrade`, пока не обновите все ноды!).

*Повторите ЭТАП 2 для второй ноды Supermicro.*

---

### ЭТАП 3: Ввод новой ноды (Huananzhi) в эксплуатацию

На эту ноду мы сразу ставим чистый Proxmox VE 8.

#### Шаг 3.1: Установка
1.  Скачайте ISO Proxmox VE 8.x.
2.  При установке выберите **128Gb NVMe** диск как target.
    *   *Filesystem:* ZFS (RAID0) или ext4 (LVM). ZFS на одном диске хорош для сжатия и проверки целостности, но ест RAM. Для системного диска ext4 тоже ок, но я рекомендую **ZFS (Single disk)**, чтобы видеть здоровье диска через Proxmox GUI.
3.  **Сеть:** Сразу задайте статический IP из вашей подсети управления (например 10.10.1.13).

#### Шаг 3.2: Настройка окружения (До ввода в кластер)
Перед объединением важно привести сетевые настройки к стандарту кластера.
1.  Зайдите в Web GUI -> System -> Network.
2.  Настройте **Linux Bond (LACP)** или **OVS Bond**, точно так же, как на старых нодах (названия бриджей `vmbr0` и VLAN теги должны совпадать!). Если имена физических интерфейсов отличаются (на Huananzhi они могут быть `enp5s0` и т.д.), просто включите их в бонд.
3.  **Time sync:** Убедитесь, что Chrony/NTP настроен на тот же сервер времени, что и остальные.

#### Шаг 3.3: Ввод в кластер
1.  На любой из старых нод (Supermicro): Datacenter -> Cluster -> **Join Information** -> Copy Information.
2.  На новой ноде (Huananzhi): Datacenter -> Cluster -> **Join Cluster** -> Вставьте информацию и пароль root от старой ноды.
3.  Нажмите Join. Связь на новой ноде пропадет на 10-30 секунд (перегенерация SSL сертификатов и настройки Corosync).

**После этого у вас должен быть кластер из 3-х нод с зелеными галочками.**

---

### ЭТАП 4: Финальная настройка Хранилищ и ZFS

Теперь настраиваем диски на новой ноде согласно обсужденной архитектуре.

1.  **Удаляем лишнее:** Зайдите в Disks -> LVM/ZFS/Directory и очистите диски, если установщик что-то создал лишнее (кроме загрузочного).
2.  **Создаем пулы (через Shell или GUI):**
    
    *   **Быстрый пул (VMs):**
        ```bash
        zpool create -f -o ashift=12 fast-vm raidz1 /dev/disk/by-id/ata-SSD1 /dev/disk/by-id/ata-SSD2 /dev/disk/by-id/ata-SSD3
        zfs set compression=lz4 fast-vm
        zfs set atime=off fast-vm
        ```
        *Добавьте его в Datacenter -> Storage как ZFS.*

    *   **Холодный пул (Backups/MinIO):**
        ```bash
        zpool create -f -o ashift=12 cold-store mirror /dev/disk/by-id/ata-HDD4TB-1 /dev/disk/by-id/ata-HDD4TB-2
        zfs set compression=zstd cold-store  # ZSTD лучше жмет холодные данные
        ```

    *   **Longhorn Disk (1TB NVMe):**
        *   Можно отформатировать в ext4 и смонтировать в `/mnt/longhorn-drive`.
        *   Либо создать ZFS Single pool `fast-db`.
        *   *Рекомендация:* Если Longhorn работает через hostpath provisioner, просто сделайте Directory storage поверх ext4/xfs. ZFS поверх ZFS (Longhorn поверх ZFS) может давать оверхед. Но для простоты управления снапшотами ZFS удобнее.

---

### ЭТАП 5: Оптимизация под Huananzhi и GPU

1.  **IOMMU (Для проброса GPU):**
    Редактируем `/etc/default/grub` на новой ноде:
    ```bash
    GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction"
    ```
    *`pcie_acs_override` часто нужен на китайских платах, так как там кривые IOMMU группы.*
    
    ```bash
    update-grub
    ```
    Добавляем модули в `/etc/modules`:
    ```
    vfio
    vfio_iommu_type1
    vfio_pci
    vfio_virqfd
    ```
    `update-initramfs -u -k all` и `reboot`.

2.  **CPU Governor:**
    Для xeon v3/v4 важно выставить режим Performance, иначе Turbo Boost может работать вяло.
    ```bash
    apt install cpufrequtils
    echo 'GOVERNOR="performance"' > /etc/default/cpufrequtils
    systemctl restart cpufrequtils
    ```

3.  **Обновление ZFS фич:**
    Теперь, когда все ноды на версии 8.x, можно обновить версии пулов на всех дисках:
    ```bash
    zpool upgrade -a
    ```
    *Внимание: После этого старый Proxmox 7.4 не сможет прочитать эти диски (пути назад нет).*

### Чек-лист проверки успеха:
- [ ] Все 3 ноды видны в GUI, статус Online.
- [ ] Версия Proxmox на всех: 8.x.x.
- [ ] Сеть работает, миграция VM между старыми и новой нодой работает.
- [ ] Новые ZFS пулы созданы и видны как Storage.
- [ ] GPU видны через `lspci -nnk` и готовы к пробросу.